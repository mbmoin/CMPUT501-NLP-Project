{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPUT501-ML-Task 2-NuSVR-Word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGYA1iu2t8tA"
      },
      "source": [
        "# **CMPUT 501 Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fppI1EWKuCvL"
      },
      "source": [
        "##ML Approach - Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR57hNxEzl7c"
      },
      "source": [
        "Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIoEPI4ZuGmp"
      },
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from scipy.stats import describe\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.svm import NuSVR\n",
        "    \n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set theme of seaborn \n",
        "sns.set()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1NdHTO2z_ij"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2tyGjd9zoTa"
      },
      "source": [
        "#upload files named \"train.csv\" \"dev.csv\" \"test.csv\" from data folder in the current runtime, by clicking the file icon on the left\n",
        "#and uploading to session storage\n",
        "train_df = pd.read_csv(\"train.csv\", dtype=str)\n",
        "dev_df = pd.read_csv(\"dev.csv\", dtype=str)\n",
        "test_df = pd.read_csv(\"test.csv\", dtype=str)\n",
        "#for development\n",
        "test_df = dev_df.copy()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gSjao3L1sxZ"
      },
      "source": [
        "y_train1 = train_df[\"meanGrade1\"].astype(float)\n",
        "y_test1 = test_df[\"meanGrade1\"].astype(float)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train2 = train_df[\"meanGrade2\"].astype(float)\n",
        "y_test2 = test_df[\"meanGrade2\"].astype(float)"
      ],
      "metadata": {
        "id": "PZn4TCXD8GhY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqrheszS2BY8"
      },
      "source": [
        "Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cOKOwKW2FFA",
        "outputId": "282a55c9-56ee-4e55-a41e-3be0d3dd3021"
      },
      "source": [
        "#Remove < and /> from original headline and create a new headline using the edited word\n",
        "train_df[\"new1\"] = train_df.apply(\n",
        "    lambda x: re.sub(r\"<.+/>\", x[\"edit1\"], x[\"original1\"]), axis=1\n",
        ")\n",
        "train_df[\"original1\"] = train_df[\"original1\"].str.replace(r\"<(.+)/>\", \"\\g<1>\")\n",
        "\n",
        "test_df[\"new1\"] = test_df.apply(\n",
        "    lambda x: re.sub(r\"<.+/>\", x[\"edit1\"], x[\"original1\"]), axis=1\n",
        ")\n",
        "test_df[\"original1\"] = test_df[\"original1\"].str.replace(r\"<(.+)/>\", \"\\g<1>\")\n",
        "\n",
        "#Load stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove < and /> from original headline and create a new headline using the edited word\n",
        "train_df[\"new2\"] = train_df.apply(\n",
        "    lambda x: re.sub(r\"<.+/>\", x[\"edit2\"], x[\"original2\"]), axis=1\n",
        ")\n",
        "train_df[\"original2\"] = train_df[\"original2\"].str.replace(r\"<(.+)/>\", \"\\g<1>\")\n",
        "\n",
        "test_df[\"new2\"] = test_df.apply(\n",
        "    lambda x: re.sub(r\"<.+/>\", x[\"edit2\"], x[\"original2\"]), axis=1\n",
        ")\n",
        "test_df[\"original2\"] = test_df[\"original2\"].str.replace(r\"<(.+)/>\", \"\\g<1>\")\n",
        "\n",
        "#Load stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "KE0Jy_7b7_Uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c7ae71-80fa-4640-e20d-0c21e2a635e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAyHkl4E2WK3"
      },
      "source": [
        "Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CLVyByx2o5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30887e7f-79b2-4ba6-e111-6e925f62ddf4"
      },
      "source": [
        "#Ref: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751\n",
        "#Ref: https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "#The below step requires some time, downloads 1662.8 MB\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J4Qu7uk3G2l"
      },
      "source": [
        "def document_vector(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in model.vocab]\n",
        "    return np.mean(model[doc], axis=0)\n",
        "\n",
        "def preprocess(text):\n",
        "    # Lowercase and Tokenize\n",
        "    doc = word_tokenize(text.lower())\n",
        "    # Remove Stop Words\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    # Remove non-alphabet tokens\n",
        "    doc = [word for word in doc if word.isalpha()]\n",
        "    return doc\n",
        "\n",
        "def to_vector(df, column):\n",
        "    headlines = df[column].tolist()\n",
        "    corpus = [preprocess(title) for title in headlines]\n",
        "    X = []\n",
        "    # append the vector for each document\n",
        "    for doc in corpus:  \n",
        "        vector = document_vector(model, doc)\n",
        "        X.append(vector)\n",
        "    return np.array(X)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiLjAdO63Mbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96772c8b-0cf9-4f6f-d375-4ad6f3a30383"
      },
      "source": [
        "nltk.download('punkt')\n",
        "X_train_orig1 = to_vector(train_df, \"original1\")\n",
        "X_train_new1 = to_vector(train_df, \"new1\")\n",
        "X_test_orig1 = to_vector(test_df, \"original1\")\n",
        "X_test_new1 = to_vector(test_df, \"new1\")\n",
        "\n",
        "#Check to make sure that the sizes match\n",
        "assert len(X_train_orig1) == len(X_train_new1) == len(train_df)\n",
        "assert len(X_test_orig1) == len(X_test_new1) == len(test_df) \n",
        "\n",
        "#Combine the vectors from the original and edited headline\n",
        "X_train1 = [\n",
        "    np.concatenate((X_train_orig1[c], X_train_new1[c]), axis=None)\n",
        "    for c in range(len(X_train_orig1))\n",
        "]\n",
        "X_test1 = [\n",
        "    np.concatenate((X_test_orig1[c], X_test_new1[c]), axis=None)\n",
        "    for c in range(len(X_test_orig1))\n",
        "]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "X_train_orig2 = to_vector(train_df, \"original2\")\n",
        "X_train_new2 = to_vector(train_df, \"new2\")\n",
        "X_test_orig2 = to_vector(test_df, \"original2\")\n",
        "X_test_new2 = to_vector(test_df, \"new2\")\n",
        "\n",
        "#Check to make sure that the sizes match\n",
        "assert len(X_train_orig2) == len(X_train_new2) == len(train_df)\n",
        "assert len(X_test_orig2) == len(X_test_new2) == len(test_df) \n",
        "\n",
        "#Combine the vectors from the original and edited headline\n",
        "X_train2 = [\n",
        "    np.concatenate((X_train_orig2[c], X_train_new2[c]), axis=None)\n",
        "    for c in range(len(X_train_orig1))\n",
        "]\n",
        "X_test2 = [\n",
        "    np.concatenate((X_test_orig2[c], X_test_new2[c]), axis=None)\n",
        "    for c in range(len(X_test_orig2))\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlh5oB43-0t5",
        "outputId": "fa431e7a-579c-4940-ccd7-d3a695ea7dcd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZmytKy3mBr"
      },
      "source": [
        "Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNMqR3vX3mzs"
      },
      "source": [
        "#Function to round the predictions to nearest even decimal since that is the form of the train set\n",
        "def roundPred(predictions):\n",
        "    final_predictions = []\n",
        "    for val in predictions:\n",
        "        val = int(val * 10)\n",
        "        final_predictions.append(val / 10)\n",
        "    return final_predictions"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38PKEF059nL"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MCeTLTt5-sj"
      },
      "source": [
        "#NuSVR\n",
        "nu_svr = NuSVR()\n",
        "nu_svr.fit(X_train1, y_train1)\n",
        "\n",
        "predictions1 = roundPred(nu_svr.predict(X_test1))\n",
        "\n",
        "sub_df = pd.DataFrame(columns=[\"id1\", \"pred1\", \"actual1\",\"id2\", \"pred2\", \"actual2\", \"labelp\", \"label\"])\n",
        "\n",
        "sub_df[\"label\"] = test_df[\"label\"]\n",
        "sub_df[\"id1\"] = test_df[\"id\"]\n",
        "sub_df[\"pred1\"] = predictions1\n",
        "sub_df[\"actual1\"] = test_df[\"meanGrade1\"]\n",
        "assert len(sub_df) == len(test_df)\n",
        "\n",
        "nu_svr.fit(X_train2, y_train2)\n",
        "predictions2 = roundPred(nu_svr.predict(X_test2))\n",
        "\n",
        "sub_df[\"id2\"] = test_df[\"id\"]\n",
        "sub_df[\"pred2\"] = predictions2\n",
        "sub_df[\"actual2\"] = test_df[\"meanGrade2\"]\n",
        "assert len(sub_df) == len(test_df)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label(x):\n",
        "   if x['pred1'] == x['pred2']:\n",
        "     x['labelp'] = 0\n",
        "   elif x['pred1'] > x['pred2']:\n",
        "     x['labelp'] = 1\n",
        "   else:\n",
        "     x['labelp'] = 2\n",
        "   return x['labelp']\n",
        "\n",
        "\n",
        "sub_df['labelp'] = sub_df.apply(label, axis=1)\n",
        "\n",
        "sub_df.to_csv(\"task2_labeling_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "S7ma_H3wRnqG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.DataFrame(columns=[\"actual\", \"predicted\"])\n",
        "final_df[\"actual\"] = sub_df[\"label\"]\n",
        "final_df[\"predicted\"] = sub_df[\"labelp\"]\n",
        "final_df.to_csv('task2_classification_results.csv', index=False)"
      ],
      "metadata": {
        "id": "8z5dqgzSTh4f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "dt = pd.read_csv('task2_classification_results.csv')\n",
        "df = pd.DataFrame(dt)\n",
        "\n",
        "gt= df[\"actual\"]\n",
        "pred = df['predicted'] \n",
        "print(\"\\nclassification_report\",classification_report(gt, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_DBKYQoMf6o",
        "outputId": "9feb9003-5291-4573-ad5c-94abc3e2409e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "classification_report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.15      0.12       256\n",
            "           1       0.55      0.53      0.54      1079\n",
            "           2       0.53      0.49      0.51      1020\n",
            "\n",
            "    accuracy                           0.47      2355\n",
            "   macro avg       0.40      0.39      0.39      2355\n",
            "weighted avg       0.49      0.47      0.48      2355\n",
            "\n"
          ]
        }
      ]
    }
  ]
}